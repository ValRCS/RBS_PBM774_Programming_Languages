{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arithmetic Parser\n",
    "\n",
    "Creating a simple arithmetic parser in Python involves several steps:\n",
    "\n",
    "- **Tokenization**: Break the input string into a list of tokens. In the case of arithmetic operations, tokens could be numbers, parentheses, or operators like `+`, `-`, `*`, `/`.\n",
    "- **Parsing**: Convert the list of tokens into an abstract syntax tree (AST). In the case of arithmetic expressions, this will involve handling operator precedence and associativity.\n",
    "- **Evaluation**: Walk through the AST and calculate the result of the expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Tokenization\n",
    "def tokenize(expression):\n",
    "    return re.findall(r\"\\d+|\\+|\\-|\\*|\\/|\\(|\\)\", expression)\n",
    "\n",
    "# Step 2: Parsing\n",
    "def parse(tokens):\n",
    "    def evaluate(tokens):\n",
    "        args = [term(tokens)]\n",
    "        while tokens and tokens[0] in \"+-\":\n",
    "            op = tokens.pop(0)\n",
    "            if op == '+':\n",
    "                args.append(term(tokens))\n",
    "            else:\n",
    "                args.append(-term(tokens))\n",
    "        return sum(args)\n",
    "\n",
    "    def term(tokens):\n",
    "        args = [factor(tokens)]\n",
    "        while tokens and tokens[0] in \"*/\":\n",
    "            op = tokens.pop(0)\n",
    "            if op == '*':\n",
    "                args.append(factor(tokens))\n",
    "            else:\n",
    "                args.append(1 / factor(tokens))\n",
    "        result = 1\n",
    "        for arg in args:\n",
    "            result *= arg\n",
    "        return result\n",
    "\n",
    "    def factor(tokens):\n",
    "        if tokens[0] == '(':\n",
    "            tokens.pop(0)  # Remove '('\n",
    "            result = evaluate(tokens)\n",
    "            tokens.pop(0)  # Remove ')'\n",
    "        else:\n",
    "            result = float(tokens.pop(0))\n",
    "        return result\n",
    "\n",
    "    return evaluate(tokens)\n",
    "\n",
    "# Step 3: Evaluation\n",
    "def evaluate_expression(expression):\n",
    "    tokens = tokenize(expression)\n",
    "    return parse(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_expression(\"2+3\")  # 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's check (2+3)*4 which should be 20\n",
    "evaluate_expression(\"(2+3)*4\")  # 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try triple parentheses (2+3)*((4+5)/3) which should be 15\n",
    "evaluate_expression(\"(2+3)*((4+5)/3)\")  # 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's add some text in between like Valdis and RBS\n",
    "# so 2 + Valdis RBS 6 should be 8\n",
    "evaluate_expression(\"2 + Valdis RBS 6\")  # 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building your own tokenizer without regular expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could make our own tokenizer without using regex but that would be more work\n",
    "# we could do it using finite state machin\n",
    "# TODO: implement own tokenizer without regex\n",
    "# let's create a tokenizer that will use finite state machine to tokenize our expression\n",
    "# we want to support parentheses, +, -, *, /, and integers\n",
    "\n",
    "# we will have states for each type of token\n",
    "# we will have a current state and transition to next state based on current character\n",
    "\n",
    "# we will have a dictionary of transitions\n",
    "# we will have a dictionary of final states\n",
    "# we will have a dictionary of token\n",
    "# we will have a dictionary of token values\n",
    "\n",
    "# we will have a function that will take a string and return a list of tokens and their values\n",
    "# if syntax is incorrect we will raise an exception\n",
    "\n",
    "# we will call this function tokenize_fsm\n",
    "\n",
    "def tokenize_fsm(raw_string):\n",
    "    # we will have a dictionary of transitions\n",
    "    transitions = {\n",
    "        \"start\": {\n",
    "            \"digit\": \"integer\",\n",
    "            \"(\": \"lparen\",\n",
    "            \")\": \"rparen\",\n",
    "            \"+\": \"plus\",\n",
    "            \"-\": \"minus\",\n",
    "            \"*\": \"mult\",\n",
    "            \"/\": \"div\",\n",
    "            \"whitespace\": \"start\"\n",
    "        },\n",
    "        \"integer\": {\n",
    "            \"digit\": \"integer\",\n",
    "            \"whitespace\": \"start\"\n",
    "        },\n",
    "        \"lparen\": {\n",
    "            \"whitespace\": \"start\"\n",
    "        },\n",
    "        \"rparen\": {\n",
    "            \"whitespace\": \"start\"\n",
    "        },\n",
    "        \"plus\": {\n",
    "            \"whitespace\": \"start\"\n",
    "        },\n",
    "        \"minus\": {\n",
    "            \"whitespace\": \"start\"\n",
    "        },\n",
    "        \"mult\": {\n",
    "            \"whitespace\": \"start\"\n",
    "        },\n",
    "        \"div\": {\n",
    "            \"whitespace\": \"start\"\n",
    "        }\n",
    "    }\n",
    "    # we will have a dictionary of final states\n",
    "    final_states = {\n",
    "        \"integer\": True,\n",
    "        \"lparen\": True,\n",
    "        \"rparen\": True,\n",
    "        \"plus\": True,\n",
    "        \"minus\": True,\n",
    "        \"mult\": True,\n",
    "        \"div\": True\n",
    "    }\n",
    "    # we will have a dictionary of token values\n",
    "    token_values = {\n",
    "        \"integer\": \"\",\n",
    "        \"lparen\": \"(\",\n",
    "        \"rparen\": \")\",\n",
    "        \"plus\": \"+\",\n",
    "        \"minus\": \"-\",\n",
    "        \"mult\": \"*\",\n",
    "        \"div\": \"/\"\n",
    "    }\n",
    "    # we will have a current state\n",
    "    current_state = \"start\"\n",
    "    # we will have a current token\n",
    "    current_token = \"\"\n",
    "    # we will have a list of tokens\n",
    "    tokens = []\n",
    "    # we will iterate over the string\n",
    "    for char in raw_string:\n",
    "        # we will determine the type of character\n",
    "        if char.isdigit():\n",
    "            char_type = \"digit\"\n",
    "        elif char == \" \": # could add support for other whitespace characters\n",
    "            char_type = \"whitespace\"\n",
    "        else:\n",
    "            char_type = char\n",
    "        # we will check if the transition is valid\n",
    "        if char_type not in transitions[current_state]:\n",
    "            raise ValueError(f\"Invalid syntax at {char}\")\n",
    "        # we will update the current state\n",
    "        current_state = transitions[current_state][char_type]\n",
    "        # we will update the current token\n",
    "        if current_state != \"start\":\n",
    "            current_token += char\n",
    "        # we will check if the current state is final\n",
    "        if current_state in final_states:\n",
    "            tokens.append((current_state, current_token))\n",
    "            current_token = \"\"\n",
    "            current_state = \"start\"\n",
    "    \n",
    "    # return [(token_values[token], value) for token, value in tokens]\n",
    "    return tokens\n",
    "\n",
    "# TODO check balance of parentheses\n",
    "# TODO digits should be combined into integers - try modifiying the code yourself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('integer', '5'), ('plus', '+'), ('integer', '9'), ('integer', '0'), ('integer', '0'), ('integer', '0')]\n"
     ]
    }
   ],
   "source": [
    "# let's get tokens for 5 + 9000\n",
    "# we should get ['5', '+', '9000']\n",
    "my_tokens = tokenize_fsm(\"5 + 9000\")\n",
    "print(my_tokens)  # [('5', '5'), ('plus', '+'), ('integer', '9000')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('lparen', '('), ('integer', '2'), ('plus', '+'), ('integer', '3'), ('rparen', ')'), ('mult', '*'), ('lparen', '('), ('lparen', '('), ('integer', '4'), ('plus', '+'), ('integer', '5'), ('integer', '0'), ('integer', '9'), ('rparen', ')'), ('div', '/'), ('integer', '3'), ('rparen', ')')]\n"
     ]
    }
   ],
   "source": [
    "# how about something more difficult such as double parentheses\n",
    "# (2+3)*((4+509)/3)\n",
    "my_tokens = tokenize_fsm(\"(2+3)*((4+509)/3)\")\n",
    "print(my_tokens)  # ['(', '2', '+', '3', ')', '*', '(', '(', '4', '+', '509', ')', '/', '3', ')']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got an exception Invalid syntax at V\n"
     ]
    }
   ],
   "source": [
    "# if we pass badly formatted string we should get an exception\n",
    "# let's try to add a letter in the middle of the expression\n",
    "try:\n",
    "    my_tokens = tokenize_fsm(\"2 + Val + 3\")\n",
    "except ValueError as e:\n",
    "    print(f\"Got an exception {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EBNF for checking syntax of our arithmetic\n",
    "\n",
    "Let's write EBNF for our arithmetic expressions:\n",
    "\n",
    "In EBNF { } are used to indicate repetition of the preceding element zero or more times, and [ ] are used to indicate optional elements.\n",
    "```\n",
    "expression = term , { (\"+\" | \"-\") , term } ;\n",
    "term = factor , { (\"*\" | \"/\"), factor } ;\n",
    "factor = number | \"(\", expression, \")\" ;\n",
    "number = \"0\" | non_zero_digit, { digit } ;\n",
    "non_zero_digit = \"1\" | \"2\" | \"3\" | \"4\" | \"5\" | \"6\" | \"7\" | \"8\" | \"9\" ;\n",
    "digit = \"0\" | non_zero_digit ;\n",
    "```\n",
    "\n",
    "TODO check the above syntax in a EBNF parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parser design decisions\n",
    "\n",
    "- This is a simple recursive-descent parser. It doesn't handle many edge cases or provide detailed error messages.\n",
    "- The tokenization step is done with a regular expression to capture digits, operators, and parentheses.\n",
    "- The parsing step converts the list of tokens into a number by recursively breaking down the terms and factors, taking into account the precedence and associativity of the operators.\n",
    "- Finally, the evaluation step is simple because we've constructed our AST such that each node immediately knows how to evaluate itself. In this simple example, the AST is implicitly built into the recursive structure of the `parse()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2+3 = 5.0\n",
      "2-3 = -1.0\n",
      "2*3 = 6.0\n",
      "6/3 = 2.0\n",
      "(2+3)*4 = 20.0\n",
      "(2+3)/5 = 1.0\n",
      "2+3*4 = 14.0\n",
      "2*3+4 = 10.0\n"
     ]
    }
   ],
   "source": [
    "test_expressions = [\n",
    "    \"2+3\",\n",
    "    \"2-3\",\n",
    "    \"2*3\",\n",
    "    \"6/3\",\n",
    "    \"(2+3)*4\",\n",
    "    \"(2+3)/5\",\n",
    "    \"2+3*4\",\n",
    "    \"2*3+4\",\n",
    "]\n",
    "for expression in test_expressions:\n",
    "    print(f\"{expression} = {evaluate_expression(expression)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
