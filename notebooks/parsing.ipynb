{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGCUcnCw7TEp"
      },
      "source": [
        "# Parsing\n",
        "\n",
        "<img alt=\"parsing\" src=\"https://upload.wikimedia.org/wikipedia/commons/d/d6/Parser_Flow%D5%B8.gif\" width=\"200\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FfFmpOW17TEs"
      },
      "source": [
        "Parsing is the process of analyzing a sequence of symbols, such as the source code of a program, in order to determine its grammatical structure with respect to a formal grammar. In the context of programming languages, parsing often serves as the phase that comes after lexical analysis (also known as scanning or tokenizing) in the compilation or interpretation process.\n",
        "\n",
        "A high-level overview of how parsing works in the context of programming languages:\n",
        "\n",
        "### Lexical Analysis\n",
        "First, the source code is broken down into a series of tokens. Tokens are the basic building blocks of a language like keywords, identifiers, literals, and various operators.\n",
        "\n",
        "### Syntax Analysis\n",
        "In the parsing phase, these tokens are then assembled in a way that reflects the program's syntax. The parser often constructs a parse tree, a hierarchical structure that represents the syntactic structure of the source code according to the formal grammar of the programming language.\n",
        "\n",
        "### Types of Parsers\n",
        "\n",
        "- **Top-Down Parsers**: These start from the root of the parse tree and try to match the input tokens to the leaves of the tree. Recursive Descent and LL parsers are examples of top-down parsers.\n",
        "- **Bottom-Up Parsers**: These start from the leaves and work their way up to the root of the parse tree. LR parsers and their variants (SLR, LALR) are examples of bottom-up parsers.\n",
        "\n",
        "### Common Algorithms\n",
        "\n",
        "- **Recursive Descent Parsing**: A very straightforward way of implementing a top-down parser by using a set of recursive procedures to process the input.\n",
        "- **LL Parsing**: A more optimized form of top-down parsing that uses a lookahead token.\n",
        "- **LR Parsing**: A type of bottom-up parsing that uses a stack to hold grammar symbols and a lookahead token. This is often used in more complex languages due to its power.\n",
        "- **Earley Parsing**: A more generalized technique that can parse any context-free grammar. It's not as efficient as specialized algorithms but has the benefit of broad applicability.\n",
        "\n",
        "### Abstract Syntax Trees\n",
        "After parsing, the parse tree is often simplified into an Abstract Syntax Tree (AST), which retains the hierarchical structure but omits some of the formal grammar details that are unnecessary for understanding the program semantics.\n",
        "\n",
        "### Semantic Analysis\n",
        "After constructing the AST, the next phase typically involves semantic analysis, where various checks like type checking, scope resolution, etc., are performed to ensure the program is semantically correct.\n",
        "\n",
        "Understanding parsing is crucial for many tasks beyond compilers, such as static code analyzers, code generators, and even some sophisticated text-editing tools. It's a fundamental topic in language design, compiler construction, and formal language theory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LXgh3iO7TEt"
      },
      "source": [
        "## Two approaches to parsing\n",
        "\n",
        "The two principal approaches to parsing in the context of programming languages are:\n",
        "\n",
        "\n",
        "- **Top-Down Parsing**: In this approach, the parsing process starts from the start symbol (the root of the parse tree) and attempts to transform it into the input string by repeatedly replacing a non-terminal symbol with its right-hand side as defined in the formal grammar. Top-down parsing tries to predict what comes next in the input based on the current set of rules and the next input token(s). Recursive descent parsers and LL parsers are common examples of top-down parsers.\n",
        "   - **Pros**: Easier to implement manually, more intuitive to understand.\n",
        "   - **Cons**: Limited in the types of grammars they can handle, and left recursion can be problematic.\n",
        "- **Bottom-Up Parsing**: This approach starts from the input tokens (the leaves of the parse tree) and attempts to construct the parse tree by repeatedly reducing sequences of tokens that match the right-hand side of some rule to the non-terminal on the left-hand side of that rule. The goal is to eventually reduce the sequence of tokens to the start symbol of the grammar. LR parsers, SLR parsers, and LALR parsers are examples of bottom-up parsers.\n",
        "   - **Pros**: Can handle a broader range of grammars compared to top-down parsers, generally more powerful.\n",
        "   - **Cons**: More complex to implement, harder to debug and diagnose errors.\n",
        "\n",
        "Both approaches have their advantages and disadvantages, and the choice between them often depends on specific requirements, such as the complexity of the language to be parsed, performance requirements, and ease of implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWtkT71y7TEu"
      },
      "source": [
        "## Lexical analysis in the context of parsing programming languages\n",
        "\n",
        "In the context of parsing programming languages, lexical analysis is the first phase of the compilation or interpretation process. The primary responsibility of this phase is to read the source code and convert it into a stream of tokens, which are the atomic building blocks of the language. These tokens are then fed into the next phase, syntax analysis or parsing, to construct the parse tree or abstract syntax tree (AST).\n",
        "\n",
        "### Key Concepts in Lexical Analysis:\n",
        "\n",
        "- **Tokens**: Tokens are the basic units in the source code that have a collective meaning. Examples include keywords like `if`, `while`, `return`; operators like `+`, `-`, `*`; identifiers; and literals like numbers or strings.\n",
        "- **Lexemes**: These are the actual character sequences in the source code that are matched to form tokens. For example, in the statement `int x = 10;`, \"int\", \"x\", \"=\", and \"10\" are lexemes that get translated into tokens: `INT_KEYWORD`, `IDENTIFIER`, `EQUALS_OPERATOR`, and `INTEGER_LITERAL`, respectively.\n",
        "- **Regular Expressions**: Often, the patterns that define tokens can be described using regular expressions. Many lexical analyzers use regular expressions or similar pattern-matching mechanisms to identify tokens.\n",
        "- **Scanner/Tokenizer**: This is the component that performs lexical analysis. It reads the source code, usually one character at a time, and groups characters into lexemes that form tokens based on the rules defined for the language.\n",
        "- **Symbol Table**: Information about some tokens, such as identifiers, may be stored in a symbol table for later phases of the compiler or interpreter to use. This helps in things like type checking, scope resolution, etc.\n",
        "\n",
        "### Steps Involved:\n",
        "\n",
        "- **Reading Input**: The scanner reads the source code either one character at a time or in larger chunks.\n",
        "- **Pattern Matching**: As the scanner reads the input, it uses predefined patterns (often based on regular expressions) to match lexemes.\n",
        "- **Token Generation**: Once a lexeme is matched, it is translated into a token that is passed on to the syntax analysis phase.\n",
        "- **Error Handling**: If the scanner encounters a sequence of characters that doesn't match any known pattern, it typically generates an error.\n",
        "- **Skipping Whitespaces and Comments**: Usually, lexical analysis also handles the removal of whitespaces, comments, and other irrelevant characters, which are not needed for the subsequent phases.\n",
        "\n",
        "By performing lexical analysis as a separate phase before syntax analysis, the parsing process becomes more manageable, modular, and efficient. The parser can focus on the structure of the code without worrying about low-level details like skipping whitespaces or combining characters into tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xw_uee1R7TEv"
      },
      "source": [
        "## Top-Down Parsing\n",
        "\n",
        "The key idea behind top-down parsing is to start from the highest-level construct of the language (often represented by the start symbol in the grammar) and work downwards, recursively breaking it into its constituent parts until the sequence of input tokens is generated. This is done according to the rules of a formal grammar that defines the syntax of the programming language.\n",
        "\n",
        "### Steps in Top-Down Parsing:\n",
        "\n",
        "- **Start Symbol**: Begin with the start symbol of the grammar as the first node in the parse tree.\n",
        "- **Token Lookahead**: Examine the next available token from the input stream to guide the parsing decision.\n",
        "- **Grammar Rule Application**: For the non-terminal symbol at hand (initially the start symbol), choose one of its production rules in the grammar that seems appropriate based on the current input token and expand that non-terminal accordingly in the parse tree.\n",
        "- **Recursion**: Continue this process, working down the tree, expanding non-terminals into their constituent parts according to the grammar and the remaining tokens to be read.\n",
        "- **Backtracking**: If a chosen production rule doesn't lead to a successful parse, backtrack to a previous decision point and try a different production rule (unless you're using a non-backtracking algorithm like LL parsing).\n",
        "- **Terminal Matching**: When you reach a terminal symbol in the grammar, it should match the next token from the input. Consume that token from the input stream and proceed.\n",
        "- **Completion**: When all input tokens have been consumed and the parse tree is successfully built, the parsing is successful.\n",
        "\n",
        "### Types of Top-Down Parsers:\n",
        "\n",
        "- **Recursive Descent Parser**: A straightforward implementation where each non-terminal in the grammar is implemented as a function. Each function inspects the next token and decides which production to expand based on it. This approach often requires backtracking and is less efficient but is simple to implement.\n",
        "- **LL Parsers**: These are a class of more efficient top-down parsers that use a fixed number of lookahead tokens to decide which production rule to use. These parsers are defined as LL(k), where `k` is the number of lookahead tokens. The most commonly used is LL(1), which uses a single lookahead token.\n",
        "\n",
        "### Advantages and Limitations:\n",
        "\n",
        "- **Advantages**:\n",
        "   - Easier to implement, especially for simpler grammars.\n",
        "   - More intuitive and easier to debug.\n",
        "   - Suitable for hand-written parsers.\n",
        "\n",
        "<li>**Limitations**:\n",
        "\n",
        "\n",
        "- Cannot handle left-recursive grammars without modification.\n",
        "- Less powerful than bottom-up parsers; they cannot parse all types of context-free grammars.\n",
        "- May require backtracking, which can be inefficient.\n",
        "\n",
        "The choice of using top-down parsing often depends on factors like the complexity of the language to be parsed, the efficiency required, and the ease of implementation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_PyUB8L7TEv"
      },
      "source": [
        "## Bottom-Up Parsing\n",
        "\n",
        "The key idea behind bottom-up parsing is to start with the input tokens and gradually reduce them into higher-level constructs according to the rules of a formal grammar. The ultimate aim is to reduce the entire string of tokens down to the start symbol of the grammar, effectively building the parse tree from the leaves up to the root.\n",
        "\n",
        "### Steps in Bottom-Up Parsing:\n",
        "\n",
        "- **Input Tokens**: Begin with a sequence of input tokens generated by lexical analysis.\n",
        "- **Stack Operations**: Maintain a stack to hold grammar symbols (both terminals and non-terminals). Initially, the stack is empty.\n",
        "- **Token Lookahead**: Use one or more lookahead tokens to decide what action to take: shifting a token onto the stack or reducing a sequence of grammar symbols on the stack to a non-terminal.\n",
        "- **Shift**: Take the next input token and \"shift\" it onto the stack. This action corresponds to recognizing that a terminal symbol should be part of some higher-level construct, although that construct may not be fully identified yet.\n",
        "- **Reduce**: When the top elements on the stack match the right-hand side of a grammar rule, \"reduce\" them by popping them off the stack and pushing the left-hand side non-terminal of the rule back onto the stack. This action corresponds to recognizing that a specific higher-level construct (non-terminal) has been found.\n",
        "- **Parsing Table**: Often, a parsing table is used to decide whether to shift or reduce (and which rule to use for reduction) based on the current stack contents and the lookahead token(s). This table is usually precomputed.\n",
        "- **Completion**: Parsing is successful if the entire input is consumed and the stack is reduced to just the start symbol.\n",
        "\n",
        "### Types of Bottom-Up Parsers:\n",
        "\n",
        "- **Simple LR (SLR)**: A simpler but less powerful form of LR parsing.\n",
        "- **Canonical LR**: The most general form of LR parsing, but also the most complex to implement.\n",
        "- **Look-Ahead LR (LALR)**: A compromise between the power of canonical LR parsing and the simplicity of SLR parsing.\n",
        "\n",
        "### Advantages and Limitations:\n",
        "\n",
        "- **Advantages**:\n",
        "   - Can handle a larger class of grammars compared to top-down parsers.\n",
        "   - Efficient parsing can be achieved, especially with table-driven methods.\n",
        "\n",
        "<li>**Limitations**:\n",
        "\n",
        "\n",
        "- More complex to implement and understand.\n",
        "- Error reporting and recovery can be more challenging compared to top-down parsers.\n",
        "\n",
        "Bottom-up parsing is generally more powerful than top-down parsing and is commonly used in automatically generated parsers, like those produced by parser generator tools such as Yacc or Bison.\n",
        "\n",
        "### Links to Yacc and Bison\n",
        "\n",
        "- [Yacc](https://en.wikipedia.org/wiki/Yacc)\n",
        "- [Bison](https://en.wikipedia.org/wiki/GNU_bison)\n",
        "\n",
        "![Bison](https://upload.wikimedia.org/wikipedia/en/thumb/2/22/Heckert_GNU_white.svg/200px-Heckert_GNU_white.svg.png)\n",
        "\n",
        "-[ANTLR](https://www.antlr.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie0L3QE27TEw"
      },
      "source": [
        "## Recursive Descent Parsing\n",
        "\n",
        "One key characteristic that distinguishes a Recursive Descent parser from other types of parsers is its straightforward implementation using recursive functions to represent the non-terminals of the grammar. Each non-terminal in the formal grammar of the language is represented by a separate function in the parser, and these functions call each other recursively to process the input tokens.\n",
        "\n",
        "### Key Aspects of Recursive Descent Parsing:\n",
        "\n",
        "- **Function for Each Non-terminal**: Each non-terminal symbol in the grammar is implemented as a function. These functions are responsible for parsing the part of the input that corresponds to their non-terminal symbols.\n",
        "- **Recursion**: These functions call each other recursively, mimicking the nested structure of the language constructs they represent. This is the \"descent\" in \"recursive descent,\" and it refers to the traversal of the parse tree from the root down to the leaves.\n",
        "- **Backtracking**: Recursive descent parsers may employ backtracking. If a chosen production rule doesn't seem to apply, the parser may backtrack to try a different rule. Note that backtracking can be avoided if the grammar is carefully designed or modified to be predictive, meaning that the next token in the input uniquely determines which production to use.\n",
        "- **Top-Down Parsing**: Recursive descent is a type of top-down parsing. It starts from the highest-level construct (the start symbol) and works its way down to the input tokens.\n",
        "- **Manual Implementation**: Recursive descent parsers are often hand-coded and are relatively easy to write and understand, making them a popular choice for simpler languages or when quick prototyping is needed.\n",
        "\n",
        "### Contrast with Other Parsers:\n",
        "\n",
        "- **LL Parsers**: Like recursive descent, LL parsers are also top-down but often use a parsing table and a stack instead of direct recursion. They can be automatically generated by tools.\n",
        "- **LR Parsers**: These are bottom-up parsers and usually rely on parsing tables and a stack. They are more complex but can handle a broader range of grammars.\n",
        "- **Earley Parsers**: These are more general parsers that can handle even some ambiguous grammars. They do not naturally fit into the top-down or bottom-up categories and are more complex to implement.\n",
        "\n",
        "In summary, the most distinguishing feature of a Recursive Descent parser is its direct mapping of grammar rules to recursive functions, which makes the approach straightforward to implement and understand but may require backtracking and may be limited in the kinds of grammars it can handle efficiently."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBLhaDBQ7TEw"
      },
      "source": [
        "## Why separate lexical analysis from syntax analysis?\n",
        "\n",
        "Separating lexical analysis from syntax analysis in the design of compilers or interpreters provides several advantages:\n",
        "\n",
        "### Simplicity and Modularity:\n",
        "\n",
        "- **Division of Labor**: By separating concerns, each phase can focus on a specific aspect of language processing, making the implementation of each simpler and more focused.\n",
        "- **Ease of Maintenance**: It becomes easier to modify or extend each component independently. For example, adding a new keyword to the language might require only a change in the lexical analyzer, leaving the syntax analyzer untouched.\n",
        "\n",
        "### Efficiency:\n",
        "\n",
        "- **Tokenization**: Lexical analysis converts the source code into a stream of tokens, which is far more straightforward to handle than the original text. This streamlining makes the syntax analysis phase faster and less error-prone.\n",
        "- **Buffering**: Lexical analyzers often read input in large chunks and produce tokens as needed, making the overall processing more efficient by reducing the number of I/O operations.\n",
        "- **Optimization**: Separate phases allow for specialized optimization techniques suited to each level of language processing.\n",
        "\n",
        "### Error Handling:\n",
        "\n",
        "- **Isolation of Errors**: Lexical and syntax errors can be detected and reported in isolation, making it easier to provide meaningful error messages and possibly recover from errors gracefully.\n",
        "- **Simpler Debugging**: When errors occur, it's generally easier to debug separate components than a single monolithic component that handles both lexical and syntax analysis.\n",
        "\n",
        "### Reusability:\n",
        "\n",
        "- **Language Tools**: The lexical analyzer can often be reused for other tools like code formatters, linters, or static analyzers, which may not require full parsing but do need tokenization.\n",
        "- **Adaptability**: It is easier to adapt or extend the compiler to handle variations or subsets of the language when lexical analysis is separated from syntax analysis.\n",
        "\n",
        "### Practicality:\n",
        "\n",
        "- **Parallel Development**: Different teams can work on lexical analysis and syntax analysis simultaneously, speeding up the development process.\n",
        "- **Generated Tools**: There are existing tools like Lex or Flex for lexical analysis and Yacc or Bison for syntax analysis that generate code based on formal descriptions. By keeping these phases separate, one can take advantage of such specialized tools.\n",
        "\n",
        "Overall, the separation of lexical and syntax analysis follows the software engineering principle of separation of concerns, leading to more robust, maintainable, and flexible systems.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y7Vbs8NM7TEw"
      },
      "source": [
        "## Lexical analysis in the context of parsing programming languages\n",
        "\n",
        "The lexical analyzer, also known as the scanner or tokenizer, is the first phase in the process of compilation or interpretation of a programming language. Its primary role is to read the source code and convert it into a stream of tokens, which are the fundamental building blocks of the language. These tokens are then passed on to subsequent phases, such as the syntax analyzer (parser), for further processing. Here is an overview of the tasks performed by a lexical analyzer:\n",
        "\n",
        "### Key Functions:\n",
        "\n",
        "- **Reading Input**: The lexical analyzer reads the source code, usually one character at a time, to identify patterns that form lexemes (string literals, identifiers, keywords, etc.).\n",
        "- **Pattern Matching**: The analyzer matches sequences of characters (lexemes) against predefined patterns to form tokens. These patterns are often described using regular expressions.\n",
        "- **Token Generation**: When a lexeme is identified, it's converted into a corresponding token. A token usually consists of a token type and optional attributes, like the value for a numeric constant.\n",
        "- **Elimination of Whitespaces and Comments**: The lexical analyzer typically removes whitespaces, comments, and other non-essential characters from the source code, as these are usually not needed for further processing by the compiler or interpreter.\n",
        "- **Symbol Table Management**: Some tokens, like identifiers or constants, may require additional information to be stored for later use in the compilation process. This information is usually kept in a symbol table.\n",
        "- **Error Handling**: If the lexical analyzer encounters a sequence of characters that doesn't match any known pattern, it generates a lexical error, such as \"unrecognized character sequence.\"\n",
        "\n",
        "### Example:\n",
        "Consider a simple line of C++ code:\n",
        "\n",
        "```cpp\n",
        "int x = 10;\n",
        "\n",
        "```\n",
        "The lexical analyzer reads this line and produces a sequence of tokens that might look something like:\n",
        "\n",
        "\n",
        "- `INT_KEYWORD`\n",
        "- `IDENTIFIER(x)`\n",
        "- `EQUALS_OPERATOR`\n",
        "- `INTEGER_LITERAL(10)`\n",
        "- `SEMICOLON`\n",
        "\n",
        "### Advantages of Lexical Analysis:\n",
        "\n",
        "- **Simplification**: By transforming the source code into tokens, lexical analysis simplifies the task of the syntax analyzer, allowing it to focus on higher-level constructs like expressions, statements, and program units.\n",
        "- **Speed and Efficiency**: Tokenization makes the parsing phase more efficient, as parsing algorithms can work more quickly with a stream of tokens than with raw source code.\n",
        "- **Error Isolation**: Lexical errors are caught early in the compilation process, simplifying debugging and error reporting.\n",
        "\n",
        "By handling these tasks, the lexical analyzer serves as a crucial component in the chain of program translation or interpretation, preparing the groundwork for the more complex operations carried out by later phases."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0K4wf_e-7TEx"
      },
      "source": [
        "## Main goals of syntax analysis\n",
        "\n",
        "Syntax analysis, also known as parsing, has two main goals in the context of compiling or interpreting a programming language:\n",
        "\n",
        "### 1. Hierarchical Structure Verification\n",
        "The first goal is to verify that the sequence of tokens generated by the lexical analysis phase forms a syntactically correct program according to the formal grammar of the programming language. In simpler terms, the parser checks that the source code uses the correct sequence of keywords, operators, and identifiers, and that it adheres to the language's rules for building valid expressions, statements, and program structures.\n",
        "\n",
        "For example, consider a simple arithmetic expression `3 + 5 * 2`. The syntax analyzer would confirm that this is a valid expression according to the rules of arithmetic expressions in the language's grammar. It checks that operators appear where they're supposed to, operands are valid, and so on.\n",
        "\n",
        "### 2. Parse Tree or Abstract Syntax Tree (AST) Generation\n",
        "The second goal is to produce a hierarchical representation of the source code, commonly in the form of a parse tree or an abstract syntax tree (AST). This tree structure captures the syntactic structure of the code, showing how different elements are related to each other according to the language's grammar.\n",
        "\n",
        "The tree is then used in subsequent phases of the compilation or interpretation process for tasks such as semantic analysis, optimization, and code generation. For example, our arithmetic expression `3 + 5 * 2` might be represented as an AST that captures the precedence of the multiplication over the addition.\n",
        "\n",
        "The hierarchical tree structure makes it easier to perform these later tasks, as the relationships between different parts of the code are explicitly captured by the tree. It is also easier to detect and report errors when the code's structure is clearly understood.\n",
        "\n",
        "In summary, syntax analysis aims to:\n",
        "\n",
        "\n",
        "- Verify that the sequence of tokens forms a syntactically correct program.\n",
        "- Produce a hierarchical tree representation that captures the syntactic structure of the source code.\n",
        "\n",
        "Achieving these goals helps pave the way for the remaining steps in the compilation or interpretation process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HT8U1LzC7TEx"
      },
      "source": [
        "## References, Sources, Books\n",
        "\n",
        "![Dragon](https://m.media-amazon.com/images/I/415Upm8yeGL._SX373_BO1,204,203,200_.jpg)\n",
        "\n",
        "- [Wikipedia](https://en.wikipedia.org/wiki/Parsing)\n",
        "- [Compiler Design in C](https://www.amazon.com/Compiler-Design-C-Allen-Holub/dp/0131550454)\n",
        "- [Dragon Book](https://www.amazon.com/Compilers-Principles-Techniques-Tools-2nd/dp/0321486811)\n",
        "- [Crafting Interpreters](https://craftinginterpreters.com/)"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "orig_nbformat": 4,
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}